{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8658224,"sourceType":"datasetVersion","datasetId":5187141}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-07T15:45:34.286335Z","iopub.execute_input":"2024-08-07T15:45:34.287340Z","iopub.status.idle":"2024-08-07T15:45:35.272862Z","shell.execute_reply.started":"2024-08-07T15:45:34.287298Z","shell.execute_reply":"2024-08-07T15:45:35.271692Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/chronic-kidney-disease-dataset-analysis/Chronic_Kidney_Dsease_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/chronic-kidney-disease-dataset-analysis/Chronic_Kidney_Dsease_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:45:55.679667Z","iopub.execute_input":"2024-08-07T15:45:55.680209Z","iopub.status.idle":"2024-08-07T15:45:55.747436Z","shell.execute_reply.started":"2024-08-07T15:45:55.680174Z","shell.execute_reply":"2024-08-07T15:45:55.746249Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:46:06.515498Z","iopub.execute_input":"2024-08-07T15:46:06.515925Z","iopub.status.idle":"2024-08-07T15:46:06.560466Z","shell.execute_reply.started":"2024-08-07T15:46:06.515887Z","shell.execute_reply":"2024-08-07T15:46:06.559289Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   PatientID  Age  Gender  Ethnicity  SocioeconomicStatus  EducationLevel  \\\n0          1   71       0          0                    0               2   \n1          2   34       0          0                    1               3   \n2          3   80       1          1                    0               1   \n3          4   40       0          2                    0               1   \n4          5   43       0          1                    1               2   \n\n         BMI  Smoking  AlcoholConsumption  PhysicalActivity  ...   Itching  \\\n0  31.069414        1            5.128112          1.676220  ...  7.556302   \n1  29.692119        1           18.609552          8.377574  ...  6.836766   \n2  37.394822        1           11.882429          9.607401  ...  2.144722   \n3  31.329680        0           16.020165          0.408871  ...  7.077188   \n4  23.726311        0            7.944146          0.780319  ...  3.553118   \n\n   QualityOfLifeScore  HeavyMetalsExposure  OccupationalExposureChemicals  \\\n0           76.076800                    0                              0   \n1           40.128498                    0                              0   \n2           92.872842                    0                              1   \n3           90.080321                    0                              0   \n4            5.258372                    0                              0   \n\n   WaterQuality  MedicalCheckupsFrequency  MedicationAdherence  \\\n0             1                  1.018824             4.966808   \n1             0                  3.923538             8.189275   \n2             1                  1.429906             7.624028   \n3             0                  3.226416             3.282688   \n4             1                  0.285466             3.849498   \n\n   HealthLiteracy  Diagnosis  DoctorInCharge  \n0        9.871449          1    Confidential  \n1        7.161765          1    Confidential  \n2        7.354632          1    Confidential  \n3        6.629587          1    Confidential  \n4        1.437385          1    Confidential  \n\n[5 rows x 54 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PatientID</th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Ethnicity</th>\n      <th>SocioeconomicStatus</th>\n      <th>EducationLevel</th>\n      <th>BMI</th>\n      <th>Smoking</th>\n      <th>AlcoholConsumption</th>\n      <th>PhysicalActivity</th>\n      <th>...</th>\n      <th>Itching</th>\n      <th>QualityOfLifeScore</th>\n      <th>HeavyMetalsExposure</th>\n      <th>OccupationalExposureChemicals</th>\n      <th>WaterQuality</th>\n      <th>MedicalCheckupsFrequency</th>\n      <th>MedicationAdherence</th>\n      <th>HealthLiteracy</th>\n      <th>Diagnosis</th>\n      <th>DoctorInCharge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>71</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>31.069414</td>\n      <td>1</td>\n      <td>5.128112</td>\n      <td>1.676220</td>\n      <td>...</td>\n      <td>7.556302</td>\n      <td>76.076800</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.018824</td>\n      <td>4.966808</td>\n      <td>9.871449</td>\n      <td>1</td>\n      <td>Confidential</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>29.692119</td>\n      <td>1</td>\n      <td>18.609552</td>\n      <td>8.377574</td>\n      <td>...</td>\n      <td>6.836766</td>\n      <td>40.128498</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.923538</td>\n      <td>8.189275</td>\n      <td>7.161765</td>\n      <td>1</td>\n      <td>Confidential</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>80</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>37.394822</td>\n      <td>1</td>\n      <td>11.882429</td>\n      <td>9.607401</td>\n      <td>...</td>\n      <td>2.144722</td>\n      <td>92.872842</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.429906</td>\n      <td>7.624028</td>\n      <td>7.354632</td>\n      <td>1</td>\n      <td>Confidential</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>40</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>31.329680</td>\n      <td>0</td>\n      <td>16.020165</td>\n      <td>0.408871</td>\n      <td>...</td>\n      <td>7.077188</td>\n      <td>90.080321</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.226416</td>\n      <td>3.282688</td>\n      <td>6.629587</td>\n      <td>1</td>\n      <td>Confidential</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>43</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.726311</td>\n      <td>0</td>\n      <td>7.944146</td>\n      <td>0.780319</td>\n      <td>...</td>\n      <td>3.553118</td>\n      <td>5.258372</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.285466</td>\n      <td>3.849498</td>\n      <td>1.437385</td>\n      <td>1</td>\n      <td>Confidential</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 54 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df.drop(['PatientID', 'DoctorInCharge'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:47:22.516651Z","iopub.execute_input":"2024-08-07T15:47:22.518796Z","iopub.status.idle":"2024-08-07T15:47:22.531145Z","shell.execute_reply.started":"2024-08-07T15:47:22.518738Z","shell.execute_reply":"2024-08-07T15:47:22.529809Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df['Diagnosis'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:47:27.243362Z","iopub.execute_input":"2024-08-07T15:47:27.244188Z","iopub.status.idle":"2024-08-07T15:47:27.261561Z","shell.execute_reply.started":"2024-08-07T15:47:27.244145Z","shell.execute_reply":"2024-08-07T15:47:27.260328Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Diagnosis\n1    1524\n0     135\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Define the features and the target\nX = df.drop('Diagnosis', axis=1)\ny = df['Diagnosis']","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:47:33.898631Z","iopub.execute_input":"2024-08-07T15:47:33.899126Z","iopub.status.idle":"2024-08-07T15:47:33.906757Z","shell.execute_reply.started":"2024-08-07T15:47:33.899079Z","shell.execute_reply":"2024-08-07T15:47:33.905451Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X = pd.get_dummies(X)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:47:38.534097Z","iopub.execute_input":"2024-08-07T15:47:38.534520Z","iopub.status.idle":"2024-08-07T15:47:38.543528Z","shell.execute_reply.started":"2024-08-07T15:47:38.534483Z","shell.execute_reply":"2024-08-07T15:47:38.542165Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:47:44.521069Z","iopub.execute_input":"2024-08-07T15:47:44.522052Z","iopub.status.idle":"2024-08-07T15:47:47.301439Z","shell.execute_reply.started":"2024-08-07T15:47:44.522001Z","shell.execute_reply":"2024-08-07T15:47:47.300334Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def apply_models(X, y):\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Check for class imbalance\n    class_counts = np.bincount(y_train)\n    if len(class_counts) > 2 or np.min(class_counts) / np.max(class_counts) < 0.1:\n      print(\"Class imbalance detected. Applying SMOTE...\")\n    \n    # Apply SMOTE (class imbalance)\n    smote = SMOTE(random_state=42)\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n    \n    # Initialize the StandardScaler\n    scaler = StandardScaler()\n\n    # Fit the scaler on the training data and transform both training and test data\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # Define the models\n    models = {\n        'LogisticRegression': LogisticRegression(),\n        'SVC': SVC(),\n        'DecisionTree': DecisionTreeClassifier(),\n        'RandomForest': RandomForestClassifier(),\n        'ExtraTrees': ExtraTreesClassifier(),\n        'AdaBoost': AdaBoostClassifier(),\n        'GradientBoost': GradientBoostingClassifier(),\n        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n        'LightGBM': LGBMClassifier(),\n        'CatBoost': CatBoostClassifier(verbose=0)\n    }\n\n    # Initialize a dictionary to hold the performance of each model\n    model_performance = {}\n\n    # Apply each model\n    for model_name, model in models.items():\n        print(f\"\\n\\033[1mClassification with {model_name}:\\033[0m\\n{'-' * 30}\")\n        \n        # Fit the model to the training data\n        model.fit(X_train, y_train)\n\n        # Make predictions on the test data\n        y_pred = model.predict(X_test)\n\n        # Calculate the accuracy and f1 score\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n\n        # Store the performance in the dictionary\n        model_performance[model_name] = (accuracy, f1)\n\n        # Print the accuracy score\n        print(\"\\033[1m**Accuracy**:\\033[0m\\n\", accuracy)\n\n        # Print the confusion matrix\n        print(\"\\n\\033[1m**Confusion Matrix**:\\033[0m\\n\", confusion_matrix(y_test, y_pred))\n\n        # Print the classification report\n        print(\"\\n\\033[1m**Classification Report**:\\033[0m\\n\", classification_report(y_test, y_pred))\n\n    # Sort the models based on f1 score and pick the top 3\n    top_3_models = sorted(model_performance.items(), key=lambda x: x[1][1], reverse=True)[:3]\n    print(\"\\n\\033[1mTop 3 Models based on F1 Score:\\033[0m\\n\", top_3_models)\n\n    # Extract the model names and classifiers for the top 3 models\n    top_3_model_names = [model[0] for model in top_3_models]\n    top_3_classifiers = [models[model_name] for model_name in top_3_model_names]\n\n    # Create a Voting Classifier with the top 3 models\n    print(\"\\n\\033[1mInitializing Voting Classifier with top 3 models...\\033[0m\\n\")\n    voting_clf = VotingClassifier(estimators=list(zip(top_3_model_names, top_3_classifiers)), voting='hard')\n    voting_clf.fit(X_train, y_train)\n    y_pred = voting_clf.predict(X_test)\n    print(\"\\n\\033[1m**Voting Classifier Evaluation**:\\033[0m\\n\")\n    print(\"\\033[1m**Accuracy**:\\033[0m\\n\", accuracy_score(y_test, y_pred))\n    print(\"\\n\\033[1m**Confusion Matrix**:\\033[0m\\n\", confusion_matrix(y_test, y_pred))\n    print(\"\\n\\033[1m**Classification Report**:\\033[0m\\n\", classification_report(y_test, y_pred))\n\n    # Create a Stacking Classifier with the top 3 models\n    print(\"\\n\\033[1mInitializing Stacking Classifier with top 3 models...\\033[0m\\n\")\n    stacking_clf = StackingClassifier(estimators=list(zip(top_3_model_names, top_3_classifiers)))\n    stacking_clf.fit(X_train, y_train)\n    y_pred = stacking_clf.predict(X_test)\n    print(\"\\n\\033[1m**Stacking Classifier Evaluation**:\\033[0m\\n\")\n    print(\"\\033[1m**Accuracy**:\\033[0m\\n\", accuracy_score(y_test, y_pred))\n    print(\"\\n\\033[1m**Confusion Matrix**:\\033[0m\\n\", confusion_matrix(y_test, y_pred))\n    print(\"\\n\\033[1m**Classification Report**:\\033[0m\\n\", classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:48:13.809767Z","iopub.execute_input":"2024-08-07T15:48:13.810419Z","iopub.status.idle":"2024-08-07T15:48:13.831347Z","shell.execute_reply.started":"2024-08-07T15:48:13.810381Z","shell.execute_reply":"2024-08-07T15:48:13.829983Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"apply_models(X,y)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T15:48:20.223455Z","iopub.execute_input":"2024-08-07T15:48:20.224460Z","iopub.status.idle":"2024-08-07T15:48:42.003415Z","shell.execute_reply.started":"2024-08-07T15:48:20.224421Z","shell.execute_reply":"2024-08-07T15:48:42.002268Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Class imbalance detected. Applying SMOTE...\n\n\u001b[1mClassification with LogisticRegression:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.8463855421686747\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  9  15]\n [ 36 272]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.20      0.38      0.26        24\n           1       0.95      0.88      0.91       308\n\n    accuracy                           0.85       332\n   macro avg       0.57      0.63      0.59       332\nweighted avg       0.89      0.85      0.87       332\n\n\n\u001b[1mClassification with SVC:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9156626506024096\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  3  21]\n [  7 301]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.30      0.12      0.18        24\n           1       0.93      0.98      0.96       308\n\n    accuracy                           0.92       332\n   macro avg       0.62      0.55      0.57       332\nweighted avg       0.89      0.92      0.90       332\n\n\n\u001b[1mClassification with DecisionTree:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.8313253012048193\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  5  19]\n [ 37 271]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.12      0.21      0.15        24\n           1       0.93      0.88      0.91       308\n\n    accuracy                           0.83       332\n   macro avg       0.53      0.54      0.53       332\nweighted avg       0.88      0.83      0.85       332\n\n\n\u001b[1mClassification with RandomForest:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9156626506024096\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  2  22]\n [  6 302]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.25      0.08      0.12        24\n           1       0.93      0.98      0.96       308\n\n    accuracy                           0.92       332\n   macro avg       0.59      0.53      0.54       332\nweighted avg       0.88      0.92      0.90       332\n\n\n\u001b[1mClassification with ExtraTrees:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9066265060240963\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  1  23]\n [  8 300]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.11      0.04      0.06        24\n           1       0.93      0.97      0.95       308\n\n    accuracy                           0.91       332\n   macro avg       0.52      0.51      0.51       332\nweighted avg       0.87      0.91      0.89       332\n\n\n\u001b[1mClassification with AdaBoost:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.8614457831325302\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[ 10  14]\n [ 32 276]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.24      0.42      0.30        24\n           1       0.95      0.90      0.92       308\n\n    accuracy                           0.86       332\n   macro avg       0.59      0.66      0.61       332\nweighted avg       0.90      0.86      0.88       332\n\n\n\u001b[1mClassification with GradientBoost:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.8885542168674698\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  6  18]\n [ 19 289]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.24      0.25      0.24        24\n           1       0.94      0.94      0.94       308\n\n    accuracy                           0.89       332\n   macro avg       0.59      0.59      0.59       332\nweighted avg       0.89      0.89      0.89       332\n\n\n\u001b[1mClassification with XGBoost:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9156626506024096\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  7  17]\n [ 11 297]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.39      0.29      0.33        24\n           1       0.95      0.96      0.95       308\n\n    accuracy                           0.92       332\n   macro avg       0.67      0.63      0.64       332\nweighted avg       0.91      0.92      0.91       332\n\n\n\u001b[1mClassification with LightGBM:\u001b[0m\n------------------------------\n[LightGBM] [Info] Number of positive: 1216, number of negative: 1216\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003132 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 2432, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9186746987951807\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  7  17]\n [ 10 298]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.41      0.29      0.34        24\n           1       0.95      0.97      0.96       308\n\n    accuracy                           0.92       332\n   macro avg       0.68      0.63      0.65       332\nweighted avg       0.91      0.92      0.91       332\n\n\n\u001b[1mClassification with CatBoost:\u001b[0m\n------------------------------\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9006024096385542\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  4  20]\n [ 13 295]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.24      0.17      0.20        24\n           1       0.94      0.96      0.95       308\n\n    accuracy                           0.90       332\n   macro avg       0.59      0.56      0.57       332\nweighted avg       0.89      0.90      0.89       332\n\n\n\u001b[1mTop 3 Models based on F1 Score:\u001b[0m\n [('LightGBM', (0.9186746987951807, 0.9121891787484275)), ('XGBoost', (0.9156626506024096, 0.9100453259985279)), ('SVC', (0.9156626506024096, 0.8992361603275849))]\n\n\u001b[1mInitializing Voting Classifier with top 3 models...\u001b[0m\n\n[LightGBM] [Info] Number of positive: 1216, number of negative: 1216\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 2432, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n\n\u001b[1m**Voting Classifier Evaluation**:\u001b[0m\n\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9156626506024096\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  6  18]\n [ 10 298]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.38      0.25      0.30        24\n           1       0.94      0.97      0.96       308\n\n    accuracy                           0.92       332\n   macro avg       0.66      0.61      0.63       332\nweighted avg       0.90      0.92      0.91       332\n\n\n\u001b[1mInitializing Stacking Classifier with top 3 models...\u001b[0m\n\n[LightGBM] [Info] Number of positive: 1216, number of negative: 1216\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003223 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 2432, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Number of positive: 972, number of negative: 973\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 1945, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499743 -> initscore=-0.001028\n[LightGBM] [Info] Start training from score -0.001028\n[LightGBM] [Info] Number of positive: 973, number of negative: 972\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002489 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 1945, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500257 -> initscore=0.001028\n[LightGBM] [Info] Start training from score 0.001028\n[LightGBM] [Info] Number of positive: 973, number of negative: 973\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 1946, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Number of positive: 973, number of negative: 973\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 1946, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Number of positive: 973, number of negative: 973\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002465 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7933\n[LightGBM] [Info] Number of data points in the train set: 1946, number of used features: 51\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n\n\u001b[1m**Stacking Classifier Evaluation**:\u001b[0m\n\n\u001b[1m**Accuracy**:\u001b[0m\n 0.9126506024096386\n\n\u001b[1m**Confusion Matrix**:\u001b[0m\n [[  5  19]\n [ 10 298]]\n\n\u001b[1m**Classification Report**:\u001b[0m\n               precision    recall  f1-score   support\n\n           0       0.33      0.21      0.26        24\n           1       0.94      0.97      0.95       308\n\n    accuracy                           0.91       332\n   macro avg       0.64      0.59      0.61       332\nweighted avg       0.90      0.91      0.90       332\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}